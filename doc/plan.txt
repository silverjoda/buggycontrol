Plan:

Monday:
- Make buggy env with randomization and trajectory generation/
- Make training script for self-supervised trajectory learning and time predictor
- Properly mount RSt265 on buggy

Tuesday:
- Gather large dataset for model learning
- Make script for maximizing target vels by optimizing ETP
- Make script for changing trajectory by optimizing ETP

- Learn to imitate random sequences with max noise frequency component of 10hz (90th percentile) in Mujoco
    . Dont forget rear wheel and steering state observers
    . Experiment with regularly sampled and distance sampled waypoints
    . Experiment with various NN architectures for imitation
- See if feature processor (at least of given trajectory) from imitator can be used for RL,
compare RL performance with imitation. Is imitated agent (self supervised viable for this job).
- Learn NN odometry from data (predict lin. acc and ang. vel)
    . See if various data balancing techniques help with learning
- Fit mujoco simulation to data
- Make visual comparison of fitted Mujoco and learned data engine and ground truth
- Compare policies learned on mujoco and learned engine on real platform
- Can we use learned model to optimize a trajectory (with geometrical constraints) slightly for faster time?
    . First try in simulation
    . If works in simulation, obtain a hokuyo and try on real platform

