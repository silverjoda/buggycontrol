# TEP traj opt
# Refactor and debug train_full_traj_tep. Parallelize traj_evaluation
# Train n_step gradient estimation
# Add spline representation for TEP
# Make new trajectory generation (using shortest path algo) for buggy based on racing and obstacle avoidance (for experiments and evaluation)
# No-regret proof for n-step gradient training?
# Try RNN and transformer as TEP
# Analyze possibilities of learning a trajectory delta regressor for faster trajectories (do we need to train by gradient? or can we do otherwise)

# Buggy agent training:
-

# MPPI
- Finish implementing mppi

# Buggy env:
- Separate env which launches buggy with random or selected maize world and shortest path traj. Implement fast cost calculation for given traj for mppi.

# Mission: Model learning and transfer
- TODO: make new buggy maize env
- TODO: cont here, debug lte engine: Train buggy on learned mujoco model and test on mujoco.
- TODO: Test mppi
- Make better trajectory generation for gataset gathering for model training
- Does training model using a dagger like method work (if default training fails).
- Make proper trajectory testing script with qualitative and quantitative metrics for comparing learned model with mujoco env (with visualization).
- Try dataset augmentation by using bilateral symmetry (Does the slow turning and forward velocity go away)
- Try balanced training to represent state with lower velocity more(somehow)
- Implement meta learning for model learning (for example using a proxy neural network with attached layers for gradient prop)
- Train RMA-style
- Compare RMA-style with trained model transfer
- Try RL with difficult initial state distribution predictor to help with difficult environments
