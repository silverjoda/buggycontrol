Plan:

# Analyze gradient modification under different prior and diffeent representation
# Read MPC racing papers
# Try to see why do-mpc crashes
# Implement MPC loss fun
# Start working on traj optimization using trained policy (train time execution, not rew, and either add noise to traj or other ideas, also consider different wp representation and spline priors)
# Mental excercise with concentration asnd memory. Also meditate
# Spanish and arabic lesson

# Mission: MPC
- Fit tyre parameters to simulator buggy data
- Make MPC controller which tracks trajectory and compare to learned policy

# Mission: Trajectory optimization using TEP
- Train TEP to predict TIME, not reward
- Plot gradient directions of trajectory optimization
- Try second order optimization
- Examine different representations for trajectory such as constant length, defined by angles.
- Use spline priors for trajectory optimization. Per point is bad

# Mission: Transfer
- Make proper trajectory testing script with qualitative and quantitative metrics for comparing learned model with mujoco env (with visualization).
- Try dataset augmentation by using bilateral symmetry (Does the slow turning and forward velocity go away)
- Try balanced training (somehow)
- Implement meta learning for model learning
- Train RMA-style
- Compare RMA-style with trained model transfer
- Try RL with difficult initial state distribution predictor to help with difficult environments

# Results:
With deviation pen: Avg test rew: [33.278557], n_visited: 36.98 (12mln, 0.99 gamma, 0.0002 lr)
Without deviation pen: sux
With small action pen we get slightly better results



