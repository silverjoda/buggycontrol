# TEP traj opt
# Figure out why TEP is not working
# In train_full_traj_tep make reward evaluation of 1 step gradient for random trajectories (Does stepping 1 gradient make trajectory better or worse).
# Make training process so that 1 gradient update trajectories are added to dataset. Theoretical working proof, al la dagger?
# Analyze possibilities of learning a trajectory delta regressor for faster trajectories (do we need to train by gradient? or can we do otherwise)

# Buggy agent training:
-

# Mission: Model learning and transfer
- Make proper trajectory testing script with qualitative and quantitative metrics for comparing learned model with mujoco env (with visualization).
- Try dataset augmentation by using bilateral symmetry (Does the slow turning and forward velocity go away)
- Try balanced training to represent state with lower velocity more(somehow)
- Try dagger-like learning where trajectory rollouts are annotated using simulator
- Implement meta learning for model learning (for example using a proxy neural network with attached layers for gradient prop)
- Train RMA-style
- Compare RMA-style with trained model transfer
- Try RL with difficult initial state distribution predictor to help with difficult environments
