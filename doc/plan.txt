# TEP traj opt
# Refactor and debug train_full_traj_tep. Parallelize traj_evaluation
# Train n_step gradient estimation
# Add spline representation for TEP
# Make new trajectory generation (using shortest path algo) for buggy based on racing and obstacle avoidance (for experiments and evaluation)
# No-regret proof for n-step gradient training?
# Try RNN and transformer as TEP
# Analyze possibilities of learning a trajectory delta regressor for faster trajectories (do we need to train by gradient? or can we do otherwise)

# Monday:
====== Buggy project =======
    - TODO: Change the barrier points from maize env, and also the way barrier loss is calculated (to make it more robust)
    - TODO: Play with scale of maize (make it a bit bigger maybe)
    - TODO: Start making model based and ilqr controllers
    - TODO: Buggy maize add to plot initial barriers to see inflated vs actual boundaries
    - TODO: Train agent with significantly more tyre friction
    - TODO: Make Field D star implementation

# Robotic transformations and frames:
Motivation: Spot example in rviz (coordinate frames form a tree, show tf tree, switch between various fixed frames: world, base_link, sensor, etc)
Defining a frame of reference. What does having a frame of reference mean (point is a linear combination of basis vectors).
Rigid body transformations (rot+trans), transforming from A to B (ref to 3b1b)
Rotation representations and conversions (euler, angle axis, quat, matrix)
Extracting individual rotation axis from transformations
Rotation example: IMU inclination from gravitational vector
Homogeneous transformations (4D)
Sensor example: Transform depth cloud from sensor to world frame
Defining a rotation frame: Manually, Using a rotation calculator, using optimization
ROS TF tree

# Spot hand:
- Try the ericvoll topics to see if hand works
- See how the high level features are represented in the API.

# Recipes:
Training buggy model on real data:
    - Find bagfiles
    - Launch statictfs.launch
    - Playback bagfile and launch buggy_model_learning_dataset_creator.py
    - Repeat for all bagfiles
    - run train_buggy_model.py with appropriate configuration

Training buggy model on mujoco data:
    - Use mujoco_dataset = train_buggy_model.ModelDataset(use_real_data=False) to make mujoco dataset
    - Use train_buggy_model.train(mujoco_dataset)

TEP training:
    - Make random trajectory dataset for TEP using traj_tep_optimizer.make_dataset() function
    - Train TEP using traj_tep_optimizer.train_tep()
    - Do 1-step gradient training using traj_tep_optimizer.train_tep_1step_grad_aggregated()

TEP testing:
    - Test TEP using traj_tep_optimizer.test_tep() function which loads and tests tep

TEP inference:
    - Load TEP
    - Use traj_tep_optimizer.optimize_env_traj(env, tep) to optimize the environment trajectory after reset

General testing procedure:
    - Train buggy model on mujoco or real data.
    - Train agent on random trajectory buggy env using train_buggy_a2c.py script (add correct env in config)
    - Train agent on maize trajectory buggy env with traj and free rewards (for later final comparison)
    - Pre-train TEP on random trajectory buggy dataset or using trajectories from maize
    - Do one-step TEP gradient training on random trajectory buggy dataset or using trajectories from maize
    - Launch full testing script to evaluate agent, agent with TEP correction, agent trained on maize with traj and free rew, MPPI with traj and free rew

# TODO: Find out why discriminator doesn't train well
# TODO: Gather mujoco data with same actions as real dataset maybe (also a plus that we will have the same amount of data)
# TODO: Check if real dataset works and if mujoco dataset gathering works, and also check the obs and act ranges
# TODO: Train discriminator and plot both umap and the other prediction accuracy plot that we had
# TODO: Make buggy env config setting so that we can choose two different env parameter vectors which have different behaviors
# TODO: Obtain dataset from sim env A
# TODO: Test hybrid env




