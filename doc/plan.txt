Plan:

# Fitting:
# Rewrite fitting process to trajectory endpoint mse, not statewise
# Optimize part of parameters, find out physical meaning of parameters (maybe optuna, with ranges?)

# MPC:
# Test MPC on singletrack model
# Test MPC with distance constraints

# TEP traj opt
# Reimplement TEP to accept angle representation for traj
# Train execution time, not reward
# Make training process so that 1 gradient update trajectories are added to dataset. Theoretical working proof?

# Buggy agent training:
Explore relative waypoint representation (with increasing distances towards the end of traj)
Explore successive angle representation for waypoints (with static or increasing distance)

# Mission: Model learning and transfer
- Make proper trajectory testing script with qualitative and quantitative metrics for comparing learned model with mujoco env (with visualization).
- Try dataset augmentation by using bilateral symmetry (Does the slow turning and forward velocity go away)
- Try balanced training to represent state with lower velocity more(somehow)
- Try dagger-like learning where trajectory rollouts are annotated using simulator
- Implement meta learning for model learning (for example using a proxy neural network with attached layers for gradient prop)
- Train RMA-style
- Compare RMA-style with trained model transfer
- Try RL with difficult initial state distribution predictor to help with difficult environments
